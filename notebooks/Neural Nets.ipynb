{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we must have the data. A sample with features, and the true value of our labels.\n",
    " * $x^{(m)}$: One sample of our data\n",
    " * $y_{t}^{(m)}$: true value of our labels\n",
    " \n",
    "Our input will be a column vector (e.g. with four features here):\n",
    "$$\\begin{bmatrix}\n",
    "    x_{1} \\\\\n",
    "    x_{2} \\\\\n",
    "    x_{3} \\\\\n",
    "    x_{4}\n",
    "  \\end{bmatrix}$$\n",
    "\n",
    "Then we need to define our hyper-parameters: Epoch and learning rate ($\\eta$).\n",
    "\n",
    "We initialize our weights ($W_{h}, W_{i}$) and biases ($b_{i}, b_{h}$). Weights refer to connections between neurons (perceptrons?) and biases refer to parameters in the neurons.\n",
    "\n",
    "We define the cost function (for example, mean squared error) that gives us our error E.\n",
    "\n",
    "Then...\n",
    "`for i in range(epoch):`\n",
    "\n",
    "We feedforward by passing our data samples into the NN\n",
    "\n",
    "Then we feedbackward by updating our weights and biases so that our error is minimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedforward:\n",
    "For a one layer neural network where $h$ is our layer and $y$ is our output:\n",
    "\n",
    "$$h^{(m)} = \\sigma\\left(W_{i}x^{(m)} + b_{i}\\right)$$\n",
    "$$y_{p}^{(m)} = \\sigma\\left(W_{h}h^{(m)} + b_{h}\\right)$$\n",
    "\n",
    "where $\\sigma(x)$ is a logistic function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function\n",
    "A function that defines our error, e.g. here is mean squared error\n",
    "$$E = \\sum_{m=1}^{N}(y_{p}^{m} - y_{t}^{m})^{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedbackward:\n",
    "Minimize error by adjusting weights and biases\n",
    "\n",
    "\n",
    "How do we update the weights to minimize the error?\n",
    "\n",
    "First we should define the cost function. for our example here the MSE is our cost function:\n",
    "\n",
    "$E= \\frac{1}{2} ({\\bf y}_t - {\\bf y}_p)^T ({\\bf y}_t - {\\bf y}_p)$\n",
    "\n",
    "We update the weight (${\\bf W}_i$ and ${\\bf W}_h$) such that the error, $E$, being minimized. The most popular algorithm is Gradient Descent:\n",
    "\n",
    "${\\bf W}_h = {\\bf W}_h + \\eta {\\partial E}/{\\partial {\\bf W}_h} $\n",
    "\n",
    "For our above example we can show that:\n",
    "\n",
    "${\\partial E}/{\\partial {\\bf W}_h} = ({\\bf y}_t - {\\bf y}_p) {\\bf y}_p (1 - {\\bf y}_p)\\bf {h}$\n",
    "\n",
    "where ${\\bf h} = \\sigma({\\bf W}_i {\\bf x}_i + {\\bf b}_i)$\n",
    "\n",
    "In above code:\n",
    "\n",
    "$D = {\\bf y}_t - {\\bf y}_p$\n",
    "\n",
    "${\\bf y}_p (1 - {\\bf y}_p)$ = slope_hidden_layer\n",
    "\n",
    "$\\bf {h}$ = hiddenlayer_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.68801785]\n",
      " [0.69347942]\n",
      " [0.62858153]]\n"
     ]
    }
   ],
   "source": [
    "# check this out:\n",
    "# https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/\n",
    "# Input array\n",
    "X=np.array([[1,0,1,0],[1,0,1,1],[0,1,0,1]])\n",
    "\n",
    "#Output\n",
    "y=np.array([[1],[1],[0]])\n",
    "\n",
    "\n",
    "#Sigmoid Function\n",
    "def sigmoid (x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "\n",
    "#Derivative of Sigmoid Function\n",
    "def derivatives_sigmoid(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "\n",
    "\n",
    "#Variable initialization\n",
    "epoch=5000 #Setting training iterations\n",
    "lr=0.1 #Setting learning rate\n",
    "inputlayer_neurons = X.shape[1] #number of features in data set\n",
    "hiddenlayer_neurons = 3 #number of hidden layers neurons\n",
    "output_neurons = 1 #number of neurons at output layer\n",
    "\n",
    "#weight and bias initialization\n",
    "wh=np.random.uniform(size=(inputlayer_neurons,hiddenlayer_neurons))\n",
    "bh=np.random.uniform(size=(1,hiddenlayer_neurons))\n",
    "wout=np.random.uniform(size=(hiddenlayer_neurons,output_neurons))\n",
    "bout=np.random.uniform(size=(1,output_neurons))\n",
    "\n",
    "\n",
    "for i in range(epoch):\n",
    "    #Forward Propogation\n",
    "    hidden_layer_input1=np.dot(X,wh)\n",
    "    hidden_layer_input=hidden_layer_input1 + bh\n",
    "    hiddenlayer_activations = sigmoid(hidden_layer_input)\n",
    "    output_layer_input1=np.dot(hiddenlayer_activations,wout)\n",
    "    output_layer_input= output_layer_input1+ bout\n",
    "    output = sigmoid(output_layer_input)\n",
    "\n",
    "    #Backpropagation\n",
    "    D = y-output\n",
    "    slope_output_layer = derivatives_sigmoid(output)\n",
    "#     slope_hidden_layer = derivatives_sigmoid(hiddenlayer_activations)\n",
    "    d_output = D * slope_output_layer\n",
    "#     Error_at_hidden_layer = d_output.dot(wout.T)\n",
    "#     d_hiddenlayer = Error_at_hidden_layer * slope_hidden_layer\n",
    "    wout += hiddenlayer_activations.T.dot(d_output) *lr\n",
    "#     bout += np.sum(d_output, axis=0,keepdims=True) *lr\n",
    "#     wh += X.T.dot(d_hiddenlayer) *lr\n",
    "#     bh += np.sum(d_hiddenlayer, axis=0,keepdims=True) *lr\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
